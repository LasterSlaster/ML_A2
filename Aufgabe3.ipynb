{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Textklassifikation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "Die Textdatenbank20 Newsgroupsfindet sich zum Download unter http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz. Benutzen Sie zum Entpacken das Python-Standardpaket tarfile. Die Datenbank enthält ca. 20.000 Textdokumente, etwa gleichmäßig aufgeteilt auf 20 Newsgroups zu unterschiedlichen Diskussionsthemen. Die Dokumente zu jeder Newsgroup befinden sich nach dem Entpacken in einem Verzeichnis gleichen Namens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import tarfile\n",
    "\n",
    "# Open Tarfile\n",
    "tar = tarfile.open(\"C:/Users/morit/iCloudDrive/MWI_3/Maschinelles_Lernen/ML_A2/20news-18828.tar.gz\")\n",
    "# Extract Tarfile if not available\n",
    "if not \"C:/Users/morit/iCloudDrive/MWI_3/Maschinelles_Lernen/ML_A2/20news-18828\":\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "Wir beschränken uns auf die vier Newsgroups alt.atheism, comp.graphics, sci.space und talk.religion.misc.  \n",
    "Lesen Sie alle Dateien aus diesen vier Verzeichnissen in eine Array von Strings ein (d.h. ein File in einem String). Speichern Sie zusätzlich die Klassenzugehörigkeit jedes Dokuments in einem Vektor ab (Kontrolle: Sie müssten jetzt 3387 Stringsim Speicher haben). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for file data\n",
    "data = []\n",
    "# list for file names\n",
    "list_files = []\n",
    "# list for file classes\n",
    "classes = []\n",
    "# relevant newsgroups\n",
    "newsgroup = [\"alt.atheism\", \"comp.graphics\", \"sci.space\", \"talk.religion.misc\"]\n",
    "# path to dir\n",
    "path = \"C:/Users/morit/iCloudDrive/MWI_3/Maschinelles_Lernen/ML_A2/20news-18828\"\n",
    "\n",
    "# iterate through filenames(directories) in dir\n",
    "for filename in os.listdir(path):\n",
    "    # choose relevant directories\n",
    "    if filename in newsgroup:\n",
    "        path_to_dir = os.path.join(path, filename)\n",
    "        # iterate through files in relevant dir\n",
    "        for file in os.listdir(path_to_dir):\n",
    "            path_to_file = os.path.join(path_to_dir, file)\n",
    "            # open file, read file, append to data list, close file\n",
    "            fd = os.open(path_to_file, os.O_RDONLY)\n",
    "            data.append(os.read(fd, 1000000).decode(\"latin-1\"))\n",
    "            os.close(fd)\n",
    "            # append file name to list for file names\n",
    "            list_files.append(file)\n",
    "            classes.append(filename)\n",
    "\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ungünstigerweise enthalten diese Dokumente immer noch den Namen der Newsgroup im Header, was eine Klassifikation trivial machen würde. Wir müssen also die ersten 2 Zeilen aus jedem Dokument entfernen, was mit folgendem Code geschieht (die Dokumente befinden sich als Strings im Array data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_header(text):\n",
    "    _before , _blankline , after = text.partition(\"\\n\\n\")\n",
    "    return after\n",
    "\n",
    "data = [strip_header(text) for text in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "Im nächsten Schritt muss jeder String in Worte (Tokens) zerlegt werden, die durch Leerzeichen, Kommas etc. voneinander getrennt sind. Hierzu setzen wir das Python-Standardpaket re ein, das zur Analyse regulärer Ausdrücke dient. Durch folgenden Befehl werden alle Tokens eines Strings txtline in einer Liste l gespeichert, nachdem er zuvor mit lower() in Kleinbuchstaben umgewandelt wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# list for all the tokens\n",
    "l_all = []\n",
    "\n",
    "for txtline in data:\n",
    "    l = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(txtline.lower())\n",
    "    for word in l:\n",
    "        l_all.append(word)\n",
    "            \n",
    "print(len(l_all))\n",
    "\n",
    "unique_tokens = set(l_all)\n",
    "len(unique_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wer sich mit regulären Ausdrücken auskennt, kann hier mit anderen Suchmustern experimentieren. \n",
    "Bestimmen Sie auf diese Weise die Größe des gemeinsamen Vokabulars aller Texte (Kontrolle: mit obigem Suchmuster müssten Sie 41777 unterschiedliche Tokens erhalten, d.h. Ihre Merkmalsvektoren sind daher 41777-dimensional). Berechnen Sie für jeden Text einen Merkmalsvektor, der für jedes Wort des Vokabulars seine Häufigkeit innerhalb des Texts enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encoder = [0] * len(unique_tokens)\n",
    "binary_bag = dict(zip(unique_tokens, hot_encoder))\n",
    "dict_all = dict(zip(list_files, data))\n",
    "merkmalsvektoren = {}\n",
    "\n",
    "for key in dict_all:\n",
    "    binary_bag = dict(zip(unique_tokens, hot_encoder))\n",
    "    l = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(dict_all[key].lower())\n",
    "    \n",
    "    for word in l:\n",
    "        #if word in unique_tokens:\n",
    "        binary_bag[word] += 1\n",
    "    merkmalsvektoren[key] = binary_bag\n",
    "\n",
    "print(len(merkmalsvektoren))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_files = pd.DataFrame({\"id\":list_files})\n",
    "df_merkmalsvektoren = pd.DataFrame(data=merkmalsvektoren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mv_t = df_merkmalsvektoren.transpose()\n",
    "df_classes = pd.DataFrame({\"class\":classes})\n",
    "df_classes = df_classes.rename(columns={\"class\":\"file_class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_classes.head())\n",
    "display(df_mv_t.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "Verwenden Sie die ersten 60% der Daten als Trainingsdatensatz, die restlichen als Testdatensatz. Trainieren Sie damit einen multinomialen naiven Bayes-Klassifikator. Bestimmen Sie den Anteil korrekter Klassifikationen auf Ihren Trainings- und Testdaten. Wie gut generalisiert Ihr Klassifikator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X = df_mv_t.to_numpy()\n",
    "y = df_classes.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state = 42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "print(clf.score(X_test, y_test))\n",
    "#print(clf.score(X_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
